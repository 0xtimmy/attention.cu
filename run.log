nvcc -o ./tests/attention/run.exe ./tests/attention/run.cu
C:\Users\Timot\git\attention.cu\include\./algebra.h(249): warning #177-D: variable "scale" was declared but never referenced
          int scale = sizea/sizeb;
              ^

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

C:\Users\Timot\git\attention.cu\include\./algebra.h(383): warning #177-D: variable "batchsize" was declared but never referenced
                  int batchsize = size / batches;
                      ^

C:\Users\Timot\git\attention.cu\include\./algebra.h(590): warning #177-D: variable "layer_idx" was declared but never referenced
          int layer_idx = idx % layersize;
              ^

C:\Users\Timot\git\attention.cu\include\./algebra.h(639): warning #177-D: variable "splitsize" was declared but never referenced
          int splitsize = size;
              ^

C:\Users\Timot\git\attention.cu\tests\attention\../../include/tensor.h(339): warning #177-D: variable "i" was declared but never referenced
              int i = 0;
                  ^

C:\Users\Timot\git\attention.cu\tests\attention\../../projects\shakespeare\model.h(211): warning #177-D: variable "k" was declared but never referenced
              Tensor* k = qkv[1];
                      ^

C:\Users\Timot\git\attention.cu\tests\attention\../../projects\shakespeare\model.h(212): warning #177-D: variable "v" was declared but never referenced
              Tensor* v = qkv[2];
                      ^

run.cu
tmpxft_00032aa4_00000000-10_run.cudafe1.cpp
   Creating library .\tests\attention\run.lib and object .\tests\attention\run.exp
python ./tests/attention/test.py
C:\Users\Timot\git\attention.cu\tests\attention\test.py:113: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\build\aten\src\ATen/core/TensorBody.h:494.)
  for s in x.grad.shape:
x
tensor([[[ 3.7673e-02, -2.1723e+00, -6.6462e-01, -4.6461e-02,  5.3714e-02,
           1.0525e+00,  1.1305e+00,  3.6165e-01, -5.2999e-02,  4.6297e-02,
           4.5855e-01, -1.2710e+00, -7.5384e-01, -8.9669e-01,  5.7544e-01,
           1.3307e+00, -9.9083e-02, -1.8248e+00,  5.4758e-01, -1.9387e-01,
          -1.4579e+00,  5.5841e-01, -5.4842e-01, -5.2894e-01,  1.0518e+00,
           5.7562e-01, -5.9898e-01,  3.3770e-01, -1.1725e+00,  1.0015e+00,
          -1.0464e+00, -1.1227e-02],
         [ 2.2597e+00, -2.0231e+00, -3.3384e-01, -2.7425e+00,  1.1807e+00,
           1.3892e+00,  2.1292e+00,  1.0651e+00, -3.8181e-01, -4.6972e-01,
           9.2203e-01, -4.7360e-01, -8.2294e-01,  7.8355e-01,  2.1711e-01,
           1.4417e+00, -1.0512e-01,  1.3430e+00, -4.7103e-01,  1.6447e+00,
           1.5726e-01,  1.5097e+00,  1.0911e+00,  1.6864e-01, -3.0804e-01,
           9.5687e-01,  7.0025e-01, -8.3159e-01, -2.2445e-01, -1.8953e-01,
          -2.5841e+00,  4.7850e-01],
         [-5.3051e-01,  1.2092e+00, -3.5857e-01, -7.8892e-02,  9.5216e-01,
           4.1902e-01, -1.4839e+00,  8.4460e-01,  1.9148e+00, -1.3276e+00,
          -6.7354e-01,  6.4581e-01, -9.2549e-01,  5.0323e-01,  3.5735e-01,
           1.1165e-01, -1.4269e-01,  5.5599e-01, -2.3086e+00, -6.9805e-01,
           2.3706e+00, -2.0288e+00,  3.6158e-01, -3.2401e-02, -2.1886e-01,
          -2.0940e-01, -1.1356e+00,  6.6911e-01, -4.4774e-01, -1.5957e+00,
           9.9660e-01,  2.4667e-02],
         [-1.4151e-01,  1.3540e+00, -2.3963e-01,  9.5387e-02,  3.9482e-01,
           9.4308e-01, -6.0044e-02, -8.7360e-01,  7.8109e-01,  1.4812e+00,
           1.6436e+00, -1.9864e+00, -4.4370e-01, -8.0328e-01,  4.0911e-01,
           5.4225e-01, -1.5279e+00, -3.4069e-01, -1.0224e+00, -1.0825e+00,
           7.3054e-01, -3.5519e+00,  1.7496e+00, -1.0614e+00,  1.7023e+00,
          -2.3960e+00, -1.3380e+00, -1.5122e+00,  9.7077e-01,  1.0715e+00,
           2.4748e-02,  6.2199e-01],
         [-5.9476e-01,  5.6780e-01,  1.2987e+00, -1.7457e-01,  1.2542e+00,
           3.4649e-01,  3.8349e-01, -1.2685e+00,  6.4475e-01,  7.5731e-01,
          -9.4346e-01,  1.5574e+00, -6.5962e-02,  8.8290e-02, -1.1015e-01,
           3.6578e-01,  2.1396e-01, -1.7826e+00,  8.8993e-01,  1.3052e+00,
          -1.4955e-01,  3.0419e-01, -1.0155e+00,  9.3884e-01, -1.2625e+00,
          -1.4096e-01,  1.8441e-01, -2.5401e+00, -5.0926e-01, -4.1380e-01,
          -1.7267e+00,  2.0040e-01],
         [ 6.6554e-02,  3.0095e-01,  1.1027e-01,  5.3793e-01,  1.2160e+00,
          -1.0446e+00, -2.3411e-01, -1.6808e-01,  1.8247e+00, -4.4277e-01,
           1.2585e-01, -1.3420e+00, -1.9822e+00,  4.3781e-01,  1.1093e+00,
           4.7474e-01, -4.2944e-02, -1.5078e+00,  1.5821e-01, -8.3117e-02,
          -3.4650e-01,  2.3409e+00,  3.3854e-01, -1.0604e+00,  1.2266e+00,
          -7.8878e-01, -3.5484e-01, -3.2198e-01,  6.5354e-01,  6.9592e-01,
          -1.9602e+00, -3.6610e-02],
         [ 6.1233e-01, -7.8448e-01,  1.3432e+00,  4.1465e-01,  1.6101e+00,
           2.3145e-01, -7.7543e-01, -1.8888e-01,  8.8999e-01,  3.1591e-02,
           2.0909e+00, -1.6208e+00,  3.3629e-01,  2.7410e+00,  9.9265e-01,
          -4.2977e-01, -1.4575e-01, -6.1891e-01,  1.0148e+00,  9.5402e-01,
          -7.8808e-01, -3.1367e-01, -2.2316e+00, -1.1785e+00, -2.1941e+00,
          -1.8562e+00,  4.0353e-02, -1.5175e+00, -2.4729e-01, -2.4362e-01,
           8.5276e-01,  3.3093e-01],
         [-1.6121e-02,  4.7621e-01,  1.0550e+00,  1.2205e+00, -1.2813e+00,
           9.5505e-02,  4.0681e-01,  4.7638e-01, -6.2584e-01,  3.8562e-01,
          -5.3418e-01,  4.1201e-01,  3.7967e-01,  8.5270e-01, -9.6092e-01,
           3.5506e-01, -1.6360e-01,  1.0210e+00,  3.5913e-01,  6.2693e-01,
          -6.7592e-01,  7.9260e-01, -8.5533e-03,  1.7865e+00,  1.3416e+00,
          -7.9472e-01, -7.0462e-01,  1.1289e+00,  9.2186e-01,  7.3302e-02,
          -1.2631e-01, -1.0764e+00],
         [-2.3577e-01,  6.7308e-01, -7.6203e-01,  7.5328e-01, -1.6885e+00,
          -8.7356e-01, -1.2815e+00,  2.9399e-01,  8.0787e-01, -1.7679e-01,
           9.0466e-01,  1.5452e+00,  4.7090e-01, -5.2988e-01,  1.3713e+00,
           1.3663e-01, -8.9955e-01, -7.7175e-01, -1.7908e+00, -7.9330e-01,
          -2.1746e-01,  7.7172e-01,  1.0025e+00, -9.4180e-01,  3.3765e-01,
          -8.3063e-02,  1.9263e-01, -1.6096e+00, -4.7343e-01, -1.5724e+00,
           2.4199e-01,  1.6815e+00],
         [ 8.5265e-01,  4.7144e-01,  1.7390e+00,  2.9697e+00,  1.5923e+00,
          -4.1353e-01,  3.3305e-01,  1.4956e-02, -2.3319e+00, -7.4281e-01,
           5.0697e-01, -3.8430e-02,  8.6447e-01,  7.2569e-02, -7.2217e-01,
           7.5675e-01,  8.5117e-01, -1.1243e+00, -1.0547e+00,  5.6035e-01,
           7.9313e-01, -1.4015e+00, -1.5941e+00,  1.9280e-01,  8.2285e-01,
           3.7468e-01, -1.3649e+00, -1.2512e+00, -7.5604e-02,  6.5008e-01,
          -6.7517e-01, -7.3998e-02],
         [ 1.2231e+00,  9.8879e-01,  8.7775e-01, -1.5849e+00, -1.3526e-01,
           4.7777e-01,  1.4929e-01, -1.1870e+00, -1.0982e+00, -9.5335e-01,
          -5.2422e-01,  2.3536e-01,  2.4002e-01, -7.8067e-01,  2.2505e+00,
           9.5375e-01,  7.9535e-01,  3.2109e-01, -7.7334e-01,  5.9566e-01,
           8.2411e-01,  2.5815e-01,  1.4504e-01, -5.9225e-01,  1.0046e+00,
          -1.2853e+00, -2.8412e-01, -1.5568e+00,  1.4089e-01, -9.6135e-01,
          -1.0401e+00, -1.1898e-01],
         [ 2.5514e-01, -4.5732e-01, -3.3906e-01,  5.2039e-02, -1.7748e+00,
          -1.1115e+00,  1.3920e-01, -1.3032e+00,  8.6490e-01,  5.7373e-02,
          -1.1228e+00, -3.0679e-01,  3.7912e-01, -9.3084e-01, -1.6054e+00,
          -1.1891e+00, -5.2552e-01, -6.6785e-01,  1.6881e-01, -1.6954e-01,
          -9.9318e-01, -6.6810e-01,  6.8121e-01, -9.5569e-02,  2.1210e+00,
          -3.4869e-01,  1.0876e+00,  4.2280e-01, -7.9151e-02, -3.0944e-03,
           5.8505e-01, -2.1002e+00],
         [ 1.2808e+00, -8.6412e-01, -2.5071e-01, -1.0047e-01,  1.8537e+00,
           8.1509e-01,  1.7384e+00,  3.9731e-01,  4.7105e-01,  1.2021e+00,
           1.0034e+00, -9.3832e-02, -9.5708e-01,  2.0855e+00, -2.2463e-01,
           8.0930e-01,  7.6504e-01, -8.3806e-01,  1.0078e+00, -8.6243e-01,
          -4.5582e-01, -1.2138e-01,  8.9494e-01,  3.9754e-01, -7.8344e-01,
           1.1781e-01, -3.1991e+00, -1.0592e+00,  7.0993e-01, -3.5880e-01,
          -9.2161e-01,  1.3700e+00],
         [ 5.3159e-01, -1.1005e-01, -1.6715e+00,  4.6280e-01,  4.9182e-01,
          -1.5555e+00,  2.0497e-01,  4.0922e-01,  6.4596e-01, -1.8678e-01,
           1.2457e+00,  4.0771e-01, -1.2599e+00,  5.5015e-01,  4.6601e-01,
           1.7227e+00,  3.6409e-01, -3.9553e-01, -3.2038e-01, -2.1235e+00,
          -5.3181e-01, -1.7734e+00, -2.9264e-01,  3.3820e-01,  1.2949e+00,
          -3.0018e-01, -8.0077e-01, -2.1063e-01,  1.5829e+00,  3.9933e-02,
           4.9556e-01, -3.1028e-01],
         [ 1.3605e+00, -1.5557e-02, -1.8978e-01,  5.9272e-01,  1.1176e+00,
           3.2136e-01,  1.1563e+00,  5.5041e-01,  7.8471e-01, -3.6197e-01,
          -1.4206e+00, -1.2390e+00, -1.1763e+00,  4.1089e-01, -4.2945e-01,
          -7.1965e-01, -1.5416e-01, -4.7011e-01, -4.6249e-01, -2.0558e-01,
           2.3815e-01,  4.9384e-01, -1.2508e+00, -1.2169e+00,  1.1884e+00,
          -1.4458e+00, -8.1429e-01,  4.7051e-01,  1.4397e+00, -1.3798e+00,
          -1.7421e-01, -1.5697e+00],
         [ 1.0037e+00,  1.5267e-01, -1.2045e+00, -1.2358e+00,  1.2973e+00,
          -5.0406e-01, -1.7903e+00,  1.2544e+00,  1.7959e+00, -5.9880e-01,
          -3.1608e-01,  3.8132e-02,  3.6824e-01, -1.0723e+00, -1.3111e+00,
          -6.0595e-01, -1.3787e+00, -3.5136e-01, -1.4845e+00, -6.5656e-01,
          -1.7354e+00, -9.8029e-01, -1.7335e-01, -1.4143e+00, -2.4655e+00,
          -5.6526e-01,  6.0549e-01,  1.0436e+00, -5.4654e-01,  9.6189e-02,
           6.9943e-01, -1.0551e+00]]], device='cuda:0',
       grad_fn=<ToCopyBackward0>)
. self attention = 
tensor([[[[-0.0602,  0.5441, -0.4264,  0.4867, -0.4650, -0.2140, -0.5017,
           -0.6380],
          [-0.0642, -0.4852, -1.2182,  0.6049,  0.0461, -1.8128, -0.4178,
            0.2325],
          [ 0.1727,  0.6139,  0.8822, -1.2192, -0.0925, -0.3018,  0.4278,
           -0.1360],
          [-1.0281, -0.4851,  0.3541, -0.0549, -0.9472,  0.6404,  0.0372,
            0.0377],
          [ 0.5223, -0.0937, -0.8222,  0.2353, -0.7955, -0.4265,  0.3516,
           -0.0275],
          [-0.4883,  0.0935, -1.3948,  0.0495,  0.1563, -0.9452, -1.1537,
            0.7407],
          [-0.5504,  0.0166, -0.6454, -0.5699, -0.8404,  0.8004,  0.0328,
            0.6066],
          [ 0.4591, -0.5320,  0.1357,  0.8644,  0.5157,  0.3595, -0.6526,
            0.3218],
          [-0.3753,  0.3992, -0.6117, -0.2913, -0.8953,  0.1519, -0.1837,
            0.8695],
          [-0.0628, -0.3614,  0.1386,  0.6154,  0.1953,  0.6575,  0.8702,
            0.2408],
          [-0.1102, -0.5521, -0.6759,  0.0624, -0.6725, -0.5187, -0.0500,
            0.3544],
          [ 0.2377, -0.1686, -0.1472, -0.3446,  0.0148,  0.4106, -0.3894,
           -0.8442],
          [ 0.3037,  0.1868, -1.1031,  0.6308, -0.2750, -0.8152, -0.2547,
           -0.2490],
          [-0.4575,  0.6610,  0.1314,  0.4825, -0.2481,  0.3534, -0.0486,
            0.0229],
          [-0.4008,  0.1529, -0.4878, -0.2631,  0.3160, -0.0695, -0.4546,
            0.0604],
          [-1.0222,  1.6113,  0.5519, -0.8586, -1.0364, -0.6689,  0.8442,
           -1.5838]],

         [[-0.0136,  0.4227,  0.3028, -0.3867, -0.2322,  0.6645, -1.0385,
           -0.8700],
          [-0.8641,  0.3785,  0.9156, -0.7364, -0.2243, -0.9229, -0.9207,
           -0.8838],
          [-0.3776,  0.2407,  0.3545, -0.3759,  0.3071, -0.9551,  0.9050,
            0.3274],
          [-0.9876,  1.1162,  0.0039, -0.7828, -0.4760, -0.4641,  0.8507,
            0.9030],
          [ 0.0944, -0.6383, -0.1426,  0.0796,  0.7563, -0.3774,  0.4089,
           -0.0773],
          [-0.1004, -0.1237,  0.2093,  0.0067, -1.2300, -0.4147,  0.3401,
           -0.4878],
          [ 0.6403, -0.4438, -0.3458, -0.8048,  0.0942,  0.6869,  0.2870,
           -0.5333],
          [-0.5629, -0.4710, -0.3860, -0.4241, -0.5554,  0.5569,  0.3307,
           -0.0751],
          [ 0.4004,  0.0808, -0.5497, -0.0191, -0.7323, -0.1513,  0.0022,
            0.9225],
          [-1.5242,  0.0916, -0.2298, -0.1806, -1.0878,  1.3598,  0.1507,
           -0.7109],
          [-0.2756,  0.0302,  0.1852, -0.6596, -1.0035, -0.6187, -0.1680,
            0.3248],
          [ 0.2320, -0.2409, -0.2969, -0.6156, -0.1013,  0.0520, -0.5955,
            0.4843],
          [-0.4909,  0.9368,  0.5245, -0.6574,  1.0430, -0.1778,  0.4671,
           -0.3053],
          [-0.0838,  0.0242, -0.1882, -0.1500, -0.6340, -0.5209,  0.3253,
            0.4035],
          [-0.3754, -0.1230,  0.1535, -0.2176, -0.7591, -0.2531,  0.1491,
           -0.4526],
          [ 0.6971, -0.4377, -0.2003, -0.1304,  0.5477, -0.6089, -0.0420,
           -0.0491]],

         [[-1.2261, -0.4873,  0.2075, -1.1119,  1.0607, -0.7778, -1.0583,
           -0.2190],
          [-0.7462,  0.3566,  0.6446, -1.0064,  0.6393, -0.3019, -0.8839,
           -0.1478],
          [ 0.9400, -0.3500, -0.3302,  1.6346, -0.3641,  0.2554,  0.6323,
           -0.8547],
          [ 0.5198, -0.5922, -0.1756,  0.0339, -0.2982,  0.7598,  0.9588,
            0.2561],
          [-0.7644,  1.1786,  0.2557, -0.6822, -0.2240,  0.6806,  0.0340,
            0.8460],
          [-0.8496, -0.4768,  0.4267,  0.7190,  0.2091, -0.0460, -0.6125,
            0.3277],
          [-1.0244,  0.5146,  0.7857,  1.1300,  1.0413,  0.1361,  0.8689,
            0.5521],
          [ 1.0053,  0.8344,  1.0172,  0.0449, -1.1355,  0.3388, -0.5443,
            0.3392],
          [-0.1948, -0.4465, -0.0069, -0.0330, -0.1142, -0.3052,  0.2986,
           -0.4476],
          [-0.2327,  0.3315,  0.6158, -0.0717, -0.8225, -1.0816,  0.6718,
            1.5857],
          [-0.2388,  0.2251,  0.0263,  0.2811, -0.3835, -0.1342, -0.5771,
            0.3813],
          [-0.1475, -0.2962,  0.0562, -0.6571, -0.3169,  0.4154, -0.6699,
            0.6389],
          [-0.6032,  0.9596,  0.4744, -1.3350,  1.1014,  0.3275,  0.4006,
           -0.0348],
          [ 0.1732,  0.3851, -0.0636, -0.7981,  0.0153, -0.3466,  0.0051,
            0.2199],
          [-0.3309,  0.1385,  1.2115,  1.0191,  0.2448,  0.2487,  0.1690,
            0.2296],
          [-0.4028,  0.7897, -0.2412,  0.2473,  0.1625,  0.3676, -0.0546,
           -0.2935]],

         [[ 0.5862, -0.0422, -0.1767,  0.1031,  0.2521,  0.7532, -0.2931,
            0.2113],
          [-0.4439, -0.5082, -0.5365, -0.5070,  0.9877,  0.0109, -0.4004,
            0.5219],
          [-0.1775, -0.2318,  0.3221,  0.1661, -0.0093,  0.3226,  0.0053,
           -1.0953],
          [ 1.2675, -0.8530,  0.8857, -0.1325, -0.7741,  0.4513, -0.0867,
            0.0344],
          [-0.4176,  1.0965, -0.5346,  0.8971, -0.6364,  0.0302, -0.2922,
           -0.0941],
          [ 0.0512,  0.1014,  0.6786,  0.2670, -0.9367,  0.4545, -0.8179,
            0.0213],
          [ 0.3981, -0.2368, -0.7619,  0.2848, -0.5419,  0.4940, -0.8141,
           -0.2671],
          [-0.1191,  0.6266, -0.5742, -0.1517, -0.2501, -0.6206,  0.3068,
            0.5450],
          [-0.1333,  0.4903,  2.0762, -0.2666, -0.2568, -0.6898, -1.0561,
            0.0111],
          [ 0.4706, -0.1834, -1.5393, -0.1940, -0.0195,  0.3100, -1.2756,
            0.4941],
          [ 0.0387, -0.4179, -0.0073, -0.7194,  0.4756,  0.5548, -0.7852,
           -0.2529],
          [ 0.1316, -0.3419,  0.1156,  0.1063, -0.2299, -0.7704,  0.8602,
           -0.3154],
          [ 0.3103,  1.1508, -0.8333,  0.3434, -1.0740,  0.3658, -0.6988,
            1.2997],
          [ 0.4202, -0.0538,  0.3985,  0.1000,  0.0979,  0.3011, -0.8786,
            0.2720],
          [-0.3886,  0.3036, -0.2648,  0.0129,  0.1556,  0.3390, -0.6801,
           -0.0980],
          [-0.0054,  0.0496,  0.4389, -0.2442,  0.3178,  0.2653,  0.1645,
           -1.3346]]]], device='cuda:0', grad_fn=<TransposeBackward0>)
Traceback (most recent call last):
  File "C:\Users\Timot\git\attention.cu\tests\attention\test.py", line 113, in <module>
    for s in x.grad.shape: 
AttributeError: 'NoneType' object has no attribute 'shape'
make: *** [testv] Error 1
